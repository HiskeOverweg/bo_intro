{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bo_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOACBTH7cPzg3r+qVUdIg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiskeOverweg/bo_intro/blob/master/bo_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyEAowGDYVhZ",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Bayesian optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2IBhfw6RQLK",
        "colab_type": "text"
      },
      "source": [
        "You can run a cell by clicking on it and pressing shift+Enter. The cell below will install some required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch2mCpaqYLAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/HiskeOverweg/bo_intro.git --upgrade\n",
        "!pip install botorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bbh74XxYLuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import botorch\n",
        "import gpytorch\n",
        "import bo_intro.datasets\n",
        "from bo_intro.run_bayesian_optimization import run_bo_experiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_flOrPJfCcJ",
        "colab_type": "text"
      },
      "source": [
        "##Finding the maximum of the sine function on the interval [0, 2$\\pi$]\n",
        "\n",
        "We can run Bayesian optimization with 1 random starting point and 20 iterations on the sine function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOTrjZtRkt0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config =  {'iterations':20, 'initial_observations':1, 'dataset':'sine', 'acquisition_function':'ei', 'noise':0}\n",
        "x, y = run_bo_experiment(config, print_progress=True, seed=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4p8ZK7MfU60",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 1** Plot a sine function and the datapoints x, y queried by the Bayesian optimization algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY3U3rMXfRaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2E9qm8QuYfM",
        "colab_type": "text"
      },
      "source": [
        "Let's fit a Gaussian process to the complete dataset. We can plot its mean and the confidence bound (2 standard deviations away from the mean)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5cS0-tguXWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_gaussian_process(x, y):\n",
        "  dataset = bo_intro.datasets.Sine()\n",
        "  x_scaled = dataset.scale(torch.from_numpy(x))\n",
        "\n",
        "  gaussian_process = botorch.models.SingleTaskGP(x_scaled, torch.from_numpy(y))\n",
        "  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood=gaussian_process.likelihood, model=gaussian_process)\n",
        "  botorch.fit.fit_gpytorch_model(mll)\n",
        "\n",
        "  x_test = torch.linspace(0, 1, 20, dtype=torch.double).unsqueeze(dim=1)\n",
        "  posterior = gaussian_process.posterior(x_test)\n",
        "  lower, upper = posterior.mvn.confidence_region()\n",
        "\n",
        "  plt.plot(dataset.rescale(x_test), posterior.mean.detach())\n",
        "  plt.plot(x, y, 'o')\n",
        "  plt.fill_between(dataset.rescale(x_test).squeeze(), lower.detach(), upper.detach(), alpha=0.5);\n",
        "  plt.xlim([0, 2*np.pi]);\n",
        "\n",
        "plot_gaussian_process(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5scYH8dB1h2c",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 2** Do you understand the shape of the confidence bound?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikoZlMgfgOMA",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 3** Try adding some noise to the observations by adapting the 'noise' value in the config dictionary. The corresponding value is the standard deviation of the Gaussian distributed noise. Plot the obtained x and y values. Is the position of the maximum close to the expected maximum at $\\pi$/2?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7d7Ca5KnN-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Gee2BLglhd",
        "colab_type": "text"
      },
      "source": [
        "##Regret\n",
        "\n",
        "The regret is defined as the difference between the true maximum of the function and the best value found so far.\n",
        "\n",
        "**Exercise 4** Plot the regret as a function of iteration number, using a logarithmic y-axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT0LbC20fwoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "running_max = np.maximum.accumulate(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXd2IyyQh6-0",
        "colab_type": "text"
      },
      "source": [
        "Since Bayesian optimization is a stochastic algorithm it can be useful to evaluate the regret over a few different initializations of the algorithm.\n",
        "\n",
        "**Exercise 5** Run the algorithm 5 times with different random seeds and make a plot of the regret as a function of iteration number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BddupzBGB2cA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd2PN-rnipUE",
        "colab_type": "text"
      },
      "source": [
        "##Comparing acquisition functions\n",
        "\n",
        "Let us now compare a few acquisition functions. You can specify the key 'acquisition_function' in the config dictionary to switch to 'random' or 'ucb' (Upper Confidence Bound).\n",
        "\n",
        "**Exercise 6** Repeat exercise 5 with a random acquisition function. Which acquisition function leads to the lowest regret?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXjrj8vQFklY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN7sQZ5jam6I",
        "colab_type": "text"
      },
      "source": [
        "##Exploring vs exploiting\n",
        "The upper confidence bound acquisition function is defined as $\\mu + \\beta \\sigma$, where $\\mu$ and $\\sigma$ are the mean and standard deviation of the Gaussian process and $\\beta$ is a constant. By increasing $\\beta$ we can make the search more explorative. The default value is $\\beta = 3$, but you can change it by specifying for instance 'beta':6 in the config dictionary.\n",
        "\n",
        "**Exercise 7** Plot a sine function and the datapoints x, y queried by the Bayesian optimization algorithm with ucb acquisition function and 'beta':6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypt2IlHp8jOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3lmGYd9imoi",
        "colab_type": "text"
      },
      "source": [
        "## Optimizing a 2-dimensional function\n",
        "\n",
        "**Exercise 8** Try optimizing the [negative Branin function](https://www.sfu.ca/~ssurjano/branin.html) by specifying 'dataset':'branin' in the config dictionary. Make a plot of regret vs iteration number (the maximum value of the negative branin function is at -0.397887)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByF4m2L_HLlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_max = -0.397887"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV0CUgfdk4F8",
        "colab_type": "text"
      },
      "source": [
        "## Template for optimization of measurement in Labber\n",
        "\n",
        "A dataset which would perform an experiment in Labber to acquire new datapoints would roughly look like (see also [Labber documentation about scripting](http://labber.org/online-doc/api/ScriptTools.html/)): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPQuR1qvk_lS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from Labber import ScriptTools\n",
        "\n",
        "class LabberExperiment(Sine):\n",
        "    def __init__(self, config={}):\n",
        "        bounds = torch.tensor([[0, 1]], dtype=torch.double)\n",
        "        self.min, _ = torch.min(bounds, dim=1, keepdim=True)\n",
        "        self.min = torch.transpose(self.min, 0, 1)\n",
        "        self.interval = torch.abs(bounds[:, 0] - bounds[:, 1])\n",
        "        self.dim = bounds.shape[0]\n",
        "        self.num_points = config.setdefault('initial_observations', 0)\n",
        "        self.x = torch.rand(self.num_points, self.dim, dtype=torch.double)\n",
        "        self.y = self.query(self.x)\n",
        "        # define measurement objects\n",
        "        sPath = os.path.dirname(os.path.abspath(__file__))\n",
        "        self.MeasResonator = ScriptTools.MeasurementObject(\\\n",
        "                os.path.join(sPath, 'TestResonator.hdf5'),\n",
        "                os.path.join(sPath, 'TestResonatorOut.hdf5'))\n",
        "        self.MeasResonator.setMasterChannel('Flux bias')\n",
        "\n",
        "    def query(self, x):\n",
        "        x_rescaled = self.rescale(x)\n",
        "        results = []\n",
        "        for setting in x:\n",
        "          self.MeasResonator.updateValue('Flux bias', setting.numpy())\n",
        "          (x,y) = self.MeasResonator.performMeasurement()\n",
        "          results.append(y)\n",
        "        return torch.tensor(results, dtype=torch.double).unsqueeze(dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXZf0HetopU4",
        "colab_type": "text"
      },
      "source": [
        "##Batch mode Bayesian optimization\n",
        "\n",
        "It is also possible to run Bayesian optimization in batch mode: rather than querying for the next most informative datapoint, we can ask for a batch of N most informative datapoints. This can be especially useful in a simulation, where you can evaluate multiple settings in parallel. More details about batch mode can be found [here](https://botorch.org/docs/batching#docsNav).\n",
        "\n",
        "##Conclusion\n",
        "\n",
        "I hope this intro helped to get a basic understanding of Bayesian optimization. If you come up with a way to use it in your own experiments, please let me know, I am curious to hear about it!"
      ]
    }
  ]
}